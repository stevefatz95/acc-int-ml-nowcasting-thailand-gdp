{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d1104e-843b-4dd4-bdc8-529d11332298",
   "metadata": {},
   "source": [
    "# Data Collection For Nowcasting Thai GDP\n",
    "\n",
    "This notebook documents the data acquisition process and contains the complete Python script used to retrieve the relevant datasets. The primary objective is to collect 16 monthly macroeconomic time series from the Bank of Thailand (BOT) database and one monthly series for the SET index from a CSV file downloaded from the Stock Exchange of Thailand (SET) website. These series will serve as predictors for the nowcasting of Thailand's GDP. The GDP data itself is sourced separately from the National Economic and Social Development Council (NESDC) website and loaded from an exported XLSX file.\n",
    "\n",
    "In the final step, data vintages are constructed to simulate a real-time forecasting environment. These prepared datasets are then exported for use in the subsequent preprocessing, modeling and evaluation stages. Additional details regarding the choice of indicators are discussed in the data chapter of the accompanying thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371b0a14-525e-4f35-993b-a52d377fd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import http.client\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed39c-639a-4c51-9525-b7daeb9c9bbf",
   "metadata": {},
   "source": [
    "## Get Data From Bank of Thailand (BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f781d633-e97e-4d49-905e-b348c1c24f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file containing multiple sheets\n",
    "file_path = \"./API_Statistic_time_series.xlsx\"\n",
    "bot_api_stats = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Initialize a list to hold all DataFrames from each sheet\n",
    "all_dfs = [] \n",
    "\n",
    "# Iterate through each sheet in the workbook\n",
    "for sheet_name, df in bot_api_stats.items():\n",
    "    df.columns = df.iloc[0]  # Set the first row as column headers \n",
    "    df = df[1:].reset_index(drop=True) # Remove the header row from data  \n",
    "    df[\"category\"] = sheet_name # Add a column to identify the source sheet\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "bot_db_stats = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Clean column names for consistency and easier manipulation\n",
    "bot_db_stats.columns = (\n",
    "    bot_db_stats.columns.str.strip() # Remove leading/trailing spaces\n",
    "    .str.replace(r\"\\s+\", \"_\", regex=True) # Replace internal spaces with underscores\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True) # Remove special characters\n",
    "    .str.lower() # Convert all column names to lowercase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19e8c99-b150-45b9-85f1-c0ed642153af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27648 entries, 0 to 27647\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   report_code_bot_website  27648 non-null  object\n",
      " 1   data_series_code         27648 non-null  object\n",
      " 2   data_series_name_th      27648 non-null  object\n",
      " 3   data_series_name_eng     27648 non-null  object\n",
      " 4   unit                     27206 non-null  object\n",
      " 5   frequency                27616 non-null  object\n",
      " 6   data_type                27616 non-null  object\n",
      " 7   description_th           27615 non-null  object\n",
      " 8   description_eng          27616 non-null  object\n",
      " 9   lag_time                 27590 non-null  object\n",
      " 10  release_schedule_th      27616 non-null  object\n",
      " 11  release_schedule_eng     27616 non-null  object\n",
      " 12  source_of_data_th        27616 non-null  object\n",
      " 13  source_of_data_eng       27595 non-null  object\n",
      " 14  observation_start        27616 non-null  object\n",
      " 15  category                 27648 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display the \"bot_db_stats\" DataFrame info\n",
    "bot_db_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d7c13d-e149-4967-b0e4-f8fe115a1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Count  Percentage\n",
      "frequency                     \n",
      "Quarterly    18628       67.38\n",
      "Monthly       7597       27.48\n",
      "Annual        1010        3.65\n",
      "M,Q,Y          332        1.20\n",
      "Daily           19        0.07\n",
      "Semi-annual     17        0.06\n",
      "Weekly          12        0.04\n",
      "Fortnightly      1        0.00\n"
     ]
    }
   ],
   "source": [
    "# Count the number of records by frequency type\n",
    "frequency_counts = bot_db_stats[\"frequency\"].value_counts()\n",
    "\n",
    "# Calculate the percentage of each frequency\n",
    "frequency_percentage = (frequency_counts / len(bot_db_stats)) * 100\n",
    "\n",
    "# Combine counts and percentages into a summary table\n",
    "frequency_summary = pd.DataFrame({\n",
    "    \"Count\": frequency_counts,\n",
    "    \"Percentage\": frequency_percentage.round(2)\n",
    "})\n",
    "\n",
    "# Display frequency distribution summary\n",
    "print(frequency_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc18712-fad6-47d5-a795-2d3ed9f8d44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter dataset to include only monthly frequency data\n",
    "bot_db_stats_filt = bot_db_stats[bot_db_stats[\"frequency\"].isin([\"Monthly\"])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088b8913-b7b3-4065-aafc-63c8674df37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually preselect series codes of interest\n",
    "presel_series_code = ['PFCG000000M000651', 'EICEIM00002', 'EILEIM00011',\n",
    "       'EIPCIM00063', 'EIPIIM00052', 'EIBSIM00060', 'EINEERM00071',\n",
    "       'EIREERM00072', 'EIEXTHBM00152', 'EIIMTHBM00155',\n",
    "       'EITRADERETM00384', 'EITRADERETM00391', 'EITRADEWHOM00426', 'EISPIM00511',\n",
    "    'EISPIM00507', 'EIBSIOTHM00445']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e7c598-fe62-4036-b233-c215387af58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further filter the economic indicators to only include \n",
    "# the variables in the 'variables' list\n",
    "bot_db_stats_filt_sub = bot_db_stats_filt[bot_db_stats_filt['data_series_code']\\\n",
    ".isin(presel_series_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b1658c-9f01-4ef2-85bd-3ae0232894bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative percentage of the observation starts by month:\n",
      "observation_start\n",
      "1994-01-01     12.50\n",
      "1999-01-01     18.75\n",
      "2000-01-01     62.50\n",
      "2008-01-01     68.75\n",
      "2009-01-01     75.00\n",
      "2010-01-01     87.50\n",
      "2011-01-01    100.00\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert to datetime format\n",
    "parsed_dates = pd.to_datetime(bot_db_stats_filt_sub[\"observation_start\"], \n",
    "                              errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Count the frequency of each monthly start date\n",
    "monthly_start_freq = parsed_dates.value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Calculate the cumulative percentage\n",
    "cumulative_percentage = monthly_start_freq.cumsum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cumulative percentage of the observation starts by month:\\n{cumulative_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11578ae7-bbe9-45c5-b4f8-fb3119d7e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fetch observations\n",
    "def fetch_observations(series_code, start_period, end_period):\n",
    "    \"\"\"\n",
    "    Fetches time series observations from the Bank of Thailand (BOT) API.\n",
    "\n",
    "    This function retrieves a specified macroeconomic time series from the BOT API \n",
    "    within a given date range. It includes built-in handling for API authentication, \n",
    "    automatic retry on rate limiting (HTTP 429) and response parsing into a clean \n",
    "    pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series_code : str\n",
    "        The BOT series code identifying the macroeconomic indicator to fetch.\n",
    "    start_period : str\n",
    "        The start date for the query in 'YYYY-MM-DD' format.\n",
    "    end_period : str\n",
    "        The end date for the query in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        A DataFrame containing the retrieved observations with columns:\n",
    "        ['series_code', 'series_name', 'date', 'value'].\n",
    "        Returns None if no data is found or if an error occurs.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If the API rate limit is exceeded (status code 429), \n",
    "        the function waits 60 seconds and retries.\n",
    "    - API responses are expected to follow the BOT's public API format.\n",
    "    - Requires an active internet connection and valid BOT API credentials.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define URL and headers\n",
    "    url = f\"https://apigw1.bot.or.th/bot/public/observations/?series_code={series_code}&start_period={start_period}&end_period={end_period}\"\n",
    "    headers = {\n",
    "        'X-IBM-Client-Id': \"insert-your-api-key\", # Insert your API key from BOT\n",
    "        'accept': \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check for rate limiting\n",
    "        if response.status_code == 429:\n",
    "            print(\"Rate limit exceeded. Sleeping for 60 seconds...\")\n",
    "            time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "            return fetch_observations(series_code, \n",
    "                                      start_period, end_period)  # Retry the request\n",
    "        \n",
    "        response.raise_for_status()  # Raise exception for 4xx or 5xx errors\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract observations if present\n",
    "        if \"result\" in data and \"series\" in data[\"result\"]:\n",
    "            series_data = data[\"result\"][\"series\"][0]  # Extract first series entry\n",
    "\n",
    "            observations = series_data.get(\"observations\", [])\n",
    "            if not observations:\n",
    "                print(f\"No observations found for {series_code}.\")\n",
    "                return None  # Return None if there are no observations\n",
    "\n",
    "            # Process the observations into a DataFrame\n",
    "            records = [\n",
    "                {\n",
    "                    \"series_code\": series_data[\"series_code\"],\n",
    "                    \"series_name\": series_data[\"series_name_eng\"],\n",
    "                    \"date\": obs[\"period_start\"],\n",
    "                    \"value\": obs[\"value\"]\n",
    "                }\n",
    "                for obs in observations\n",
    "            ]\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            return pd.DataFrame(records)\n",
    "        \n",
    "        else:\n",
    "            print(f\"No data found for {series_code}\")\n",
    "            return None\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {series_code}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4b2f60-a502-4ad2-9a3e-efdd6890c2de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store all valid series' fetched data\n",
    "all_valid_series_data = []\n",
    "\n",
    "# Ensure proper column naming\n",
    "bot_db_stats_filt_sub = bot_db_stats_filt_sub\\\n",
    ".rename(columns={\"data_series_code\": \"series_code\"})\n",
    "\n",
    "# Loop through all unique series codes and fetch their observations\n",
    "for series_code in bot_db_stats_filt_sub.series_code.unique():\n",
    "    try:\n",
    "        # Try fetching observations\n",
    "        result = fetch_observations(\n",
    "            series_code, \n",
    "            start_period=\"2011-01-01\", \n",
    "            end_period=\"2019-12-31\"\n",
    "        )\n",
    "        \n",
    "        # Check if result is not None and is a DataFrame\n",
    "        if result is not None and not result.empty:\n",
    "            # Optionally, add the series_code as a column to track source\n",
    "            result['series_code'] = series_code\n",
    "            # Append to list\n",
    "            all_valid_series_data.append(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {series_code}: {e}\")\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "if all_valid_series_data:\n",
    "    bot_db_stats_final = pd.concat(all_valid_series_data, ignore_index=True)\n",
    "else:\n",
    "    bot_db_stats_final = pd.DataFrame()  # Empty fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2052dd84-7f5c-4194-80c6-babbe5566a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata columns (data_type, lag_time, release_schedule) into final dataset\n",
    "bot_db_stats_final = bot_db_stats_final.merge(\n",
    "    bot_db_stats_filt_sub[[\"series_code\", \"data_type\", \n",
    "                           \"lag_time\", \"release_schedule_eng\"]],\n",
    "    on=\"series_code\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3b5115-f6cf-423a-85ec-5fe0437c4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for getting release dates for Vintage creation\n",
    "\n",
    "# Import library for date offsetting\n",
    "import pandas.tseries.offsets as offsets\n",
    "\n",
    "# Load the Thai Financial Holidays\n",
    "thai_hol = pd.read_csv(\"thai-financial-institutions-holidays.csv\")\n",
    "\n",
    "# Convert holiday 'date' column to datetime data type\n",
    "thai_hol['date'] = pd.to_datetime(thai_hol['date'], format=\"%d-%m-%Y\")\n",
    "\n",
    "# Get holiday dates as a set for fast lookup\n",
    "holiday_dates = set(thai_hol['date'].dt.date)\n",
    "\n",
    "# Convert 'date' in main dataset to datetime and set to month end\n",
    "bot_db_stats_final['date'] = pd.to_datetime(bot_db_stats_final['date'], \n",
    "                                            format='%Y-%m') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "\n",
    "# Define the holiday and weekend adjustment function\n",
    "def adjust_for_holiday_and_weekend(rel_date, holiday_dates, \n",
    "                                   is_first_business_day=False):\n",
    "    \"\"\"\n",
    "    Adjusts a release date to the nearest valid business day.\n",
    "    \n",
    "    If `is_first_business_day=True`, moves forward to the next business day.\n",
    "    Otherwise, moves backward to the most recent business day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rel_date: Initial release date\n",
    "    holiday_dates: Set of public holidays\n",
    "    is_first_business_day: Whether to move forward or backward\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Adjusted business day (datetime)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        is_holiday = rel_date.date() in holiday_dates\n",
    "        is_weekend = rel_date.weekday() >= 5  # 5 = Saturday, 6 = Sunday\n",
    "\n",
    "        if not is_holiday and not is_weekend:\n",
    "            break\n",
    "\n",
    "        if is_first_business_day:\n",
    "            # Move forward to the next business day for the first business day\n",
    "            rel_date += offsets.BDay(1)\n",
    "        else:\n",
    "            # Move backward to the previous business day for other cases\n",
    "            rel_date -= offsets.BDay(1)\n",
    "\n",
    "    return rel_date\n",
    "\n",
    "# Define function for calculating the release dates\n",
    "def calculate_release_date(row, holiday_dates):\n",
    "    \"\"\"\n",
    "    Calculates the release date for a given time series record.\n",
    "\n",
    "    This function determines the actual release date based on the reference period,\n",
    "    the specified release schedule (e.g., first or last business day of a future month),\n",
    "    and any additional lag. It also adjusts for Thai financial holidays and weekends\n",
    "    to ensure the release date falls on the correct business day.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A row from a DataFrame representing a time series record. Must contain:\n",
    "        - 'date': pd.Timestamp indicating the end of the reference month.\n",
    "        - 'release_schedule_eng': str describing the release schedule in English.\n",
    "        - 'lag_time': str or None, indicating lag (e.g., '1 month', '2 months').\n",
    "    holiday_dates : list of datetime.date\n",
    "        A list of dates representing Thai financial market holidays. Used to\n",
    "        adjust the release date to the nearest valid business day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Timestamp or pd.NaT\n",
    "        The adjusted release date that complies with the specified schedule and\n",
    "        avoids holidays and weekends. Returns `pd.NaT` if the schedule is\n",
    "        unrecognized or improperly specified.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function supports common schedule descriptions such as:\n",
    "        * \"The first business day of the following month\"\n",
    "        * \"The last business day of the following month\"\n",
    "        * \"The last business day of the second month after the reference period\"\n",
    "    - Unknown or unsupported schedule strings will result in `pd.NaT`.\n",
    "    - Lag time is interpreted in months and defaults to one month if ambiguous.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    adjust_for_holiday_and_weekend : Helper function used to align dates with business days.\n",
    "    \"\"\"\n",
    "    ref_date = row['date']\n",
    "    schedule = row['release_schedule_eng']\n",
    "    lag = str(row.get('lag_time'))\\\n",
    "    .lower() if pd.notnull(row.get('lag_time')) else None\n",
    "\n",
    "    if schedule == 'The first business day of the following month':\n",
    "        # Determine lag: default = no lag => next month's first business day\n",
    "        if lag == '2 month' or lag == '2 months':\n",
    "            ref_date += offsets.MonthEnd(2)\n",
    "        elif lag == '1 month':\n",
    "            ref_date += offsets.MonthEnd(1)\n",
    "        # if lag is None (no lag), use ref_date as-is\n",
    "        \n",
    "        # Move to first business day **after** that date\n",
    "        rel_date = ref_date + offsets.BDay(1)\n",
    "        rel_date = adjust_for_holiday_and_weekend(rel_date, holiday_dates, \n",
    "                                                  is_first_business_day=True)\n",
    "\n",
    "    elif schedule == 'The last business day of the second month after the reference period':\n",
    "        ref_date += offsets.MonthEnd(2)\n",
    "        rel_date = adjust_for_holiday_and_weekend(ref_date, holiday_dates, \n",
    "                                                  is_first_business_day=False)\n",
    "\n",
    "    elif schedule in ['The last business day of the following month',\n",
    "                      'The last business day of the next month',\n",
    "                      'The last business day']:\n",
    "        if lag == '1 month':\n",
    "            ref_date += offsets.MonthEnd(1)\n",
    "        elif lag is None:\n",
    "            pass  # use ref_date directly\n",
    "        else:\n",
    "            ref_date += offsets.MonthEnd(1)  # fallback to 1 month for other/unexpected lags\n",
    "\n",
    "        rel_date = adjust_for_holiday_and_weekend(ref_date, holiday_dates, \n",
    "                                                  is_first_business_day=False)\n",
    "\n",
    "    else:\n",
    "        return pd.NaT  # Unknown schedule\n",
    "\n",
    "    return rel_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886ae7c5-3de7-4925-bbc2-71fc1a30727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the release date calculation to each row\n",
    "bot_db_stats_final['release_date'] = bot_db_stats_final.apply(\n",
    "    lambda row: calculate_release_date(row, holiday_dates), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348dea47-f613-46f2-8216-785ad187adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate (series_code, date) combinations found in the final dataset.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate date entries within each series\n",
    "duplicates = bot_db_stats_final[bot_db_stats_final.duplicated(subset=['series_code', \n",
    "                                                                      'date'], \n",
    "                                                              keep=False)]\n",
    "\n",
    "# Display duplicates if found\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicate rows with the same series_code and date.\")\n",
    "else:\n",
    "    print(\"No duplicate (series_code, date) combinations found in the final dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a558df06-6a5c-4dbe-9887-9809985499e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1728 entries, 0 to 1727\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   series_code           1728 non-null   object        \n",
      " 1   series_name           1728 non-null   object        \n",
      " 2   date                  1728 non-null   datetime64[ns]\n",
      " 3   value                 1728 non-null   object        \n",
      " 4   data_type             1728 non-null   object        \n",
      " 5   lag_time              1512 non-null   object        \n",
      " 6   release_schedule_eng  1728 non-null   object        \n",
      " 7   release_date          1728 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), object(6)\n",
      "memory usage: 108.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check info before exporting\n",
    "bot_db_stats_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c04da68b-e5df-4111-a8c5-8f656d188f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export BOT dataset to csv\n",
    "# Uncomment if needed\n",
    "#bot_db_stats_final.to_csv(\"bot_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ddd90-1b55-49cb-b1a0-49d3f9ffc8a3",
   "metadata": {},
   "source": [
    "## Get Financial Data From SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb9ebcc-ed80-428e-aaab-02f7158717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SET Index data\n",
    "set_df = pd.read_csv(\"set_index.csv\", skiprows=3)\n",
    "\n",
    "# Drop any rows where the first column is \"Base\" (unit information)\n",
    "set_df = set_df[set_df.iloc[:, 0] != \"Base\"]\n",
    "\n",
    "# Select only the first two columns (Date and Value) and limit to expected row count\n",
    "set_df = set_df.iloc[:601, [0, 1]]\n",
    "\n",
    "# Rename columns to more meaningful names\n",
    "set_df.columns = [\"date_raw\", \"value_raw\"]\n",
    "\n",
    "# Convert to datetime format with end-of-month alignment\n",
    "set_df[\"date\"] = pd.to_datetime(set_df[\"date_raw\"], format=\"%b-%Y\", errors=\"coerce\")\n",
    "set_df[\"date\"] = set_df[\"date\"] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Convert the value column to numeric, removing commas\n",
    "set_df[\"value\"] = pd.to_numeric(set_df[\"value_raw\"].astype(str).str.replace(\",\", \"\"), \n",
    "                                errors=\"coerce\")\n",
    "\n",
    "# Add required columns to match bot_db_stats_final\n",
    "set_df[\"series_code\"] = \"SETINDEXTHA\"\n",
    "set_df[\"series_name\"] = \"SET Index\"\n",
    "set_df[\"data_type\"] = \"Index\"\n",
    "set_df[\"lag_time\"] = np.nan\n",
    "set_df[\"release_schedule_eng\"] = np.nan\n",
    "set_df[\"release_date\"] = pd.NaT\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_set_final = set_df[[\n",
    "    \"series_code\", \"series_name\", \"date\", \"value\",\n",
    "    \"data_type\", \"lag_time\", \"release_schedule_eng\", \"release_date\"\n",
    "]]\n",
    "\n",
    "# Drop rows with missing dates or values\n",
    "df_set_final = df_set_final.dropna(subset=[\"date\", \"value\"])\n",
    "\n",
    "# Filter the period from 2011Q1 to 2019Q4\n",
    "df_set_final = df_set_final[df_set_final[\"date\"].between(\"2011-01-01\", \"2019-12-31\")]\n",
    "\n",
    "# Reindex to ensure complete end-of-month frequency using 'ME'\n",
    "full_index = pd.date_range(start=\"2011-01-31\", end=\"2019-12-31\", freq=\"ME\")\n",
    "df_set_final = df_set_final.set_index(\"date\").reindex(full_index)\n",
    "\n",
    "# Set frequency explicitly for compatibility with X-13\n",
    "df_set_final.index.freq = \"ME\"\n",
    "\n",
    "# Restore date column\n",
    "df_set_final = df_set_final.reset_index().rename(columns={\"index\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e3bddd-9646-4895-abfc-2cc5e9e0143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108 entries, 0 to 107\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   date                  108 non-null    datetime64[ns]\n",
      " 1   series_code           108 non-null    object        \n",
      " 2   series_name           108 non-null    object        \n",
      " 3   value                 108 non-null    float64       \n",
      " 4   data_type             108 non-null    object        \n",
      " 5   lag_time              0 non-null      float64       \n",
      " 6   release_schedule_eng  0 non-null      float64       \n",
      " 7   release_date          0 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(3), object(3)\n",
      "memory usage: 6.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check df_set_final info\n",
    "df_set_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4847c500-d624-4c99-b750-98cbfff70838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the data to get final dataset\n",
    "final_df = pd.concat([bot_db_stats_final, df_set_final], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abda814f-ad8e-4ab6-9587-bed4445c6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataset (information set)\n",
    "# Uncomment if needed\n",
    "#final_df.to_csv(\"set_bot_dataset_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf6f99-3023-4472-b9ff-92ac94ebbacd",
   "metadata": {},
   "source": [
    "## Get Quarterly GDP Growth Rate from NESDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce59274e-c9ef-42c0-9ed3-442bba9677b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NESDC Excel file\n",
    "file_path = \"./article_file_20250217085437.xlsx\"\n",
    "nesdc_gdp = pd.read_excel(file_path, sheet_name=\"Table 2.2\", skiprows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e27438ac-06e6-4e60-a714-44a6c4bf4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year (4-digit number at the start of the string), \n",
    "# even with suffix like '2021r'\n",
    "nesdc_gdp['Year'] = (\n",
    "    nesdc_gdp['Unnamed: 0']\n",
    "    .astype(str)\n",
    "    .str.extract(r'(?P<year>\\d{4})')['year']\n",
    "    .ffill()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Filter rows that represent quarters (e.g., 'Q1', 'Q2', etc.)\n",
    "quarter_mask = nesdc_gdp['Unnamed: 0'].astype(str).str.contains(r'^Q[1-4]', na=False)\n",
    "quarterly_gdp = nesdc_gdp[quarter_mask].copy()\n",
    "\n",
    "# Extract the quarter from the 'Unnamed: 0' column, \n",
    "# assuming it's in the format Q1, Q2, etc.\n",
    "quarterly_gdp['Quarter'] = quarterly_gdp['Unnamed: 0'].astype(str)\\\n",
    ".str.extract(r'(Q[1-4])')[0]\n",
    "\n",
    "# Combine year and quarter into 'YYYYQx' format\n",
    "quarterly_gdp['quarter'] = quarterly_gdp['Year']\\\n",
    ".astype(str) + quarterly_gdp['Quarter']\n",
    "\n",
    "# Select and rename the GDP growth column\n",
    "quarterly_gdp_final = quarterly_gdp[['(11)', 'quarter']].copy()\n",
    "quarterly_gdp_final.rename(columns={'(11)': 'gdp_growth'}, inplace=True)\n",
    "\n",
    "# Filter for the range from 2011-Q1 to 2019-Q4\n",
    "quarterly_gdp_final = quarterly_gdp_final[\n",
    "    (quarterly_gdp_final['quarter'] >= '2011Q1') & \n",
    "    (quarterly_gdp_final['quarter'] <= '2019Q4')\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a646cef-f5fe-4ccf-880c-fa3a24b2329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace full-revised 2019Q4 GDP data with the preliminary one\n",
    "# Source: https://www.nesdc.go.th/nesdb_en/article_attach/article_file_20200221170629.pdf\n",
    "quarterly_gdp_final.loc[quarterly_gdp_final.index[-1], 'gdp_growth'] = 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e59e5b1-bece-452a-a9a8-bf022fd96cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   gdp_growth  36 non-null     float64\n",
      " 1   quarter     36 non-null     object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 704.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Show final dataframe info\n",
    "quarterly_gdp_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5c6c7dd-8569-47ac-8520-8fb42231425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned target data\n",
    "# Uncomment if needed\n",
    "#quarterly_gdp_final.to_csv(\"thai-gdp-quarterly-2011Q1-2019Q4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

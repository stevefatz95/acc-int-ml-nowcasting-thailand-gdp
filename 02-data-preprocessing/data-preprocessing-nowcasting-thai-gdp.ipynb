{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778d1d2e-84c3-4b27-81fb-8502dbacbf42",
   "metadata": {},
   "source": [
    "# Data Preprocessing For Nowcasting Thai GDP\n",
    "\n",
    "This notebook details the steps involved in preprocessing the collected data. The objective of this stage is to transform the raw datasets obtained from the `dataset-collection-thai-gdp-nowcasting.ipynb` notebook into a clean, consistent and suitable format for subsequent modeling. All details on the preprocessing steps can be found in the data chapter of the accompanying thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d21e8a3-4555-4ed2-b28a-29fe91106d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76079e2b-5da0-4506-956d-7a2707bd68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "bot_set_df = pd.read_csv(\"./set_bot_dataset_final.csv\")\n",
    "gdp_df = pd.read_csv(\"./thai-gdp-quarterly-2011Q1-2019Q4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04535eb5-0663-4e51-8023-a45f78dda49d",
   "metadata": {},
   "source": [
    "## Create Pseudo Real-Time Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe489c09-e60e-4c71-b517-0368921be0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to simulate a real-time dataset\n",
    "def get_real_time_dataset(df, cutoff_date=\"2020-01-31\"):\n",
    "    \"\"\"\n",
    "    Simulate a real-time dataset by removing values whose release date is after a given cutoff.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): Input dataframe with 'release_date' and 'value' columns.\n",
    "    cutoff_date (str or datetime): The date before which data is considered known.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: Real-time simulated dataframe with a new 'value_real_time' column.\n",
    "    \"\"\"\n",
    "    # Make a copy of original dataset\n",
    "    df = df.copy()\n",
    "    # Convert to datetime\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create a new column for real-time simulation\n",
    "    df['value_real_time'] = df['value']\n",
    "    \n",
    "    # Censor future data (simulate only knowing data available as of cutoff_date)\n",
    "    df.loc[df['release_date'] > pd.to_datetime(cutoff_date), 'value_real_time'] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96e8a2a-825c-478b-9b6a-d065cbe61c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get real-time dataset\n",
    "rt_df = get_real_time_dataset(bot_set_df, \n",
    "                              \"2020-01-31\" # Cutoff date\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2204350d-978b-46ed-9372-efb4bde11f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series: Total Government Expenditure | Missing Values in 'value': 0\n",
      "Series: Import Volume Index (exclude Gold) | Missing Values in 'value': 0\n",
      "Series: Export Volume Index (exclude Gold) | Missing Values in 'value': 0\n",
      "Series: Private Consumption Index (Seasonally Adjusted)  | Missing Values in 'value': 0\n",
      "Series: Private Investment Index (PII) (Seasonally Adjusted) | Missing Values in 'value': 0\n",
      "Series: Business Sentiment Index of Investment | Missing Values in 'value': 0\n",
      "Series: Nominal Effective Exchange Rate (NEER) | Missing Values in 'value': 1\n",
      "Series: Real Effective Exchange Rate (REER) | Missing Values in 'value': 2\n",
      "Series: Export Value Index (THB) | Missing Values in 'value': 0\n",
      "Series: Import Value Index (THB) | Missing Values in 'value': 0\n",
      "Series: Retail Sales Index | Missing Values in 'value': 1\n",
      "Series: Retail Sales Index Durable Goods | Missing Values in 'value': 1\n",
      "Series: Wholesales Index Durable Goods | Missing Values in 'value': 1\n",
      "Series: Other Business Sentiment Indices Export conditions | Missing Values in 'value': 0\n",
      "Series: Service Production Index Wholesale and retail trade | Missing Values in 'value': 0\n",
      "Series: Service Production Index Real estate, renting and business activities | Missing Values in 'value': 0\n",
      "Series: SET Index | Missing Values in 'value': 0\n"
     ]
    }
   ],
   "source": [
    "# Loop through each unique series in rt_df and print missing values count\n",
    "# This is for verification purposes\n",
    "for series in rt_df['series_name'].unique():\n",
    "    df_series = rt_df[rt_df['series_name'] == series].copy()\n",
    "    missing_values_count = df_series['value_real_time'].isna().sum()\n",
    "    print(f\"Series: {series} | Missing Values in 'value': {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdee9a-bb74-4a29-9f1e-89a00cfafd36",
   "metadata": {},
   "source": [
    "## Seasonal Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca4ae4e-c701-4381-a310-688f1e70227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import X13 ARIMA adjustment method\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis\n",
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Define helper function to check seasonally adjusted label\n",
    "def is_seasonally_adjusted(series_name):\n",
    "    \"\"\"\n",
    "    Determines whether a series is already seasonally adjusted based on its name.\n",
    "\n",
    "    The check is based on common suffixes such as 'seasonally adjusted' or 'sa',\n",
    "    ignoring case and trailing punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series_name : str\n",
    "        The name of the time series to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the series appears to be seasonally adjusted, False otherwise.\n",
    "    \"\"\"\n",
    "    name = series_name.lower().strip()\n",
    "    name = re.sub(r'[\\s\\.\\)\\-]+$', '', name)  # Remove trailing punctuation/spaces\n",
    "    return name.endswith('seasonally adjusted') or name.endswith('sa')\n",
    "\n",
    "# Define function to apply X-13 ARIMA seasonal adjustment\n",
    "def apply_x13(series):\n",
    "    \"\"\"\n",
    "    Apply the X-13 ARIMA seasonal adjustment to a time series, dropping NaNs temporarily.\n",
    "    Returns the adjusted series aligned with the original index (including NaNs).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series (pd.Series): The time series data (should be in pandas Series format with DateTime index).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series: The seasonally adjusted time series with NaNs preserved.\n",
    "    If X-13 fails, the original series is returned as a fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the original index\n",
    "        original_index = series.index\n",
    "\n",
    "        # Drop NaNs for adjustment\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        # Apply X-13 ARIMA seasonal adjustment\n",
    "        res = x13_arima_analysis(series_clean, freq='M', outlier=True, \n",
    "                                 x12path='/usr/local/bin/x13as')\n",
    "\n",
    "        # Extract adjusted values\n",
    "        seasadj = res.seasadj\n",
    "\n",
    "        # Create full series with original index (NaNs preserved)\n",
    "        adjusted_series = pd.Series(index=original_index, dtype='float64')\n",
    "        adjusted_series.loc[series_clean.index] = seasadj\n",
    "\n",
    "        return adjusted_series\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying X-13 ARIMA adjustment: {e}\")\n",
    "        return series  # Return unadjusted if adjustment fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2185268d-06f9-45e8-b770-d13d17f3b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for seasonal adjustment\n",
    "def seasonal_adjust(df, value_col='value_real_time', date_col='date', series_col='series_name'):\n",
    "    \"\"\"\n",
    "    Apply X-13 ARIMA seasonal adjustment to a multi-series DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    df (pd.DataFrame): DataFrame with at least columns: [date_col, value_col, series_col]\n",
    "    value_col (str): Column name with the values to adjust (default: 'value')\n",
    "    date_col (str): Column name with the date (default: 'date')\n",
    "    series_col (str): Column name with the series name (default: 'series_name')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: Same DataFrame with a new 'sea_adj' column containing adjusted values.\n",
    "    \"\"\"\n",
    "    # Initialize list to hold adjusted series\n",
    "    adjusted_dfs = []\n",
    "\n",
    "    # Process each series individually\n",
    "    for series in df[series_col].unique():\n",
    "        # Subset the DataFrame for the current series\n",
    "        df_series = df[df[series_col] == series].copy()\n",
    "\n",
    "        # Check whether the series needs seasonal adjustment\n",
    "        if not is_seasonally_adjusted(series):\n",
    "            print(f\"[ADJUSTING] {series}\")\n",
    "            # Apply X-13 ARIMA seasonal adjustment\n",
    "            adjusted_series = apply_x13(df_series.set_index(date_col)[value_col])\n",
    "            df_series['sea_adj'] = adjusted_series.values\n",
    "        else:\n",
    "            print(f\"[SKIPPING] Already adjusted: {series}\")\n",
    "            # Series already adjusted â€” just copy the original values\n",
    "            df_series['sea_adj'] = df_series[value_col]\n",
    "\n",
    "        # Append result to list\n",
    "        adjusted_dfs.append(df_series)\n",
    "\n",
    "    result_df = pd.concat(adjusted_dfs).sort_values(by=date_col).reset_index(drop=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0add4fd2-7717-412a-bbeb-682bf8cbf86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Total Government Expenditure\n",
      "[ADJUSTING] Import Volume Index (exclude Gold)\n",
      "[ADJUSTING] Export Volume Index (exclude Gold)\n",
      "[SKIPPING] Already adjusted: Private Consumption Index (Seasonally Adjusted) \n",
      "[SKIPPING] Already adjusted: Private Investment Index (PII) (Seasonally Adjusted)\n",
      "[ADJUSTING] Business Sentiment Index of Investment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant seasonal peak has been found\n",
      "          in the estimated spectrum of the regARIMA residuals.\n",
      "  warn(errors, X13Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Nominal Effective Exchange Rate (NEER)\n",
      "[ADJUSTING] Real Effective Exchange Rate (REER)\n",
      "[ADJUSTING] Export Value Index (THB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: Visually significant seasonal and trading day peaks have \n",
      "          been found in the estimated spectrum of the regARIMA residuals.\n",
      "  warn(errors, X13Warning)\n",
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: Visually significant seasonal and trading day peaks have \n",
      "          been found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n",
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: Visually significant seasonal and trading day peaks have \n",
      "          been found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Import Value Index (THB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n",
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Retail Sales Index\n",
      "[ADJUSTING] Retail Sales Index Durable Goods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Wholesales Index Durable Goods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n",
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADJUSTING] Other Business Sentiment Indices Export conditions\n",
      "[ADJUSTING] Service Production Index Wholesale and retail trade\n",
      "[ADJUSTING] Service Production Index Real estate, renting and business activities\n",
      "[ADJUSTING] SET Index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanograssi/miniconda3/envs/py-ds/lib/python3.10/site-packages/statsmodels/tsa/x13.py:192: X13Warning: WARNING: Visually significant seasonal and trading day peaks have \n",
      "          been found in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant trading day peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    }
   ],
   "source": [
    "# Apply seasonal adjustment\n",
    "seasonally_adjusted_df = seasonal_adjust(rt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657b3e7-2580-42d1-9ed6-65fd07ced46b",
   "metadata": {},
   "source": [
    "## Transformation and Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fceb68-cd2d-4f47-9382-8b57fd13380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Augmented Dickey-Fuller (ADF) test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Define helper function for applying stationarity transformation\n",
    "def apply_stationarity_transformation(df, series_column='series_name', value_column='sea_adj'):\n",
    "    \"\"\"\n",
    "    Apply stationarity transformations to each time series in the DataFrame.\n",
    "\n",
    "    This function performs the Augmented Dickey-Fuller (ADF) test for each time series.\n",
    "    If the series is non-stationary, it applies first or second differencing as needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): Input DataFrame with time series data.\n",
    "    series_column (str): The name of the column containing the series names.\n",
    "    value_column (str): The name of the column containing the time series values to be tested.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with the stationary series and the applied transformations.\n",
    "    \"\"\"\n",
    "    stationary_dfs = []  # List to store processed DataFrames\n",
    "\n",
    "    # Iterate over each unique series\n",
    "    for series in df[series_column].unique():\n",
    "        # Extract data for the current series\n",
    "        df_series = df[df[series_column] == series].copy()\n",
    "\n",
    "        # Store full index and original values\n",
    "        full_index = df_series.index\n",
    "        original_series = df_series[value_column]\n",
    "\n",
    "        # Drop NA values and select the value column for ADF test\n",
    "        series_data = original_series.dropna()\n",
    "\n",
    "        # Perform ADF test to check for stationarity\n",
    "        pval = adfuller(series_data)[1]\n",
    "\n",
    "        if pval > 0.05:\n",
    "            # Apply first differencing if the series is non-stationary\n",
    "            stat_series = series_data.diff()\n",
    "\n",
    "            # Re-test after first differencing\n",
    "            pval_diff1 = adfuller(stat_series.dropna())[1]\n",
    "\n",
    "            if pval_diff1 > 0.05:\n",
    "                # Apply second differencing if still non-stationary\n",
    "                stat_series = stat_series.diff()\n",
    "                transform = 'diff2'\n",
    "            else:\n",
    "                transform = 'diff1'\n",
    "        else:\n",
    "            # No differencing needed if already stationary\n",
    "            stat_series = series_data\n",
    "            transform = 'none'\n",
    "\n",
    "        # Create a full-length series aligned with original index (NaNs where needed)\n",
    "        stat_aligned = pd.Series(index=series_data.index, dtype='float64')\n",
    "        stat_aligned.loc[stat_series.index] = stat_series\n",
    "\n",
    "        # Reindex to full index to preserve alignment (including NaNs)\n",
    "        stat_aligned = stat_aligned.reindex(full_index)\n",
    "\n",
    "        # Assign the transformation type to a new column\n",
    "        df_series['stationarity_transform'] = transform\n",
    "\n",
    "        # Log the transformation applied to the current series\n",
    "        print(f\"[ADF] {series}: Transformation applied = {transform}\")\n",
    "\n",
    "        # Assign the stationary series to a new column\n",
    "        df_series['stat_value'] = stat_aligned.values\n",
    "\n",
    "        # Append the transformed series to the list\n",
    "        stationary_dfs.append(df_series)\n",
    "\n",
    "    # Combine all transformed series into a single DataFrame\n",
    "    return pd.concat(stationary_dfs).sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb36a757-d57a-446e-b5b0-8fee68a6c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADF] Total Government Expenditure: Transformation applied = none\n",
      "[ADF] Nominal Effective Exchange Rate (NEER): Transformation applied = diff1\n",
      "[ADF] Retail Sales Index Durable Goods: Transformation applied = none\n",
      "[ADF] Service Production Index Real estate, renting and business activities: Transformation applied = diff1\n",
      "[ADF] Export Volume Index (exclude Gold): Transformation applied = none\n",
      "[ADF] Retail Sales Index: Transformation applied = diff1\n",
      "[ADF] Real Effective Exchange Rate (REER): Transformation applied = diff1\n",
      "[ADF] Import Volume Index (exclude Gold): Transformation applied = none\n",
      "[ADF] SET Index: Transformation applied = diff1\n",
      "[ADF] Business Sentiment Index of Investment: Transformation applied = none\n",
      "[ADF] Service Production Index Wholesale and retail trade: Transformation applied = diff1\n",
      "[ADF] Private Consumption Index (Seasonally Adjusted) : Transformation applied = diff1\n",
      "[ADF] Import Value Index (THB): Transformation applied = diff1\n",
      "[ADF] Export Value Index (THB): Transformation applied = diff1\n",
      "[ADF] Private Investment Index (PII) (Seasonally Adjusted): Transformation applied = diff1\n",
      "[ADF] Other Business Sentiment Indices Export conditions: Transformation applied = none\n",
      "[ADF] Wholesales Index Durable Goods: Transformation applied = diff1\n"
     ]
    }
   ],
   "source": [
    "# Get stationary series\n",
    "stationary_df = apply_stationarity_transformation(seasonally_adjusted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "045df2d8-b220-4be1-a3fc-76921e663d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series: Total Government Expenditure | Missing Values in 'stat_value': 0\n",
      "Series: Real Effective Exchange Rate (REER) | Missing Values in 'stat_value': 3\n",
      "Series: Private Consumption Index (Seasonally Adjusted)  | Missing Values in 'stat_value': 1\n",
      "Series: Other Business Sentiment Indices Export conditions | Missing Values in 'stat_value': 0\n",
      "Series: Retail Sales Index Durable Goods | Missing Values in 'stat_value': 1\n",
      "Series: Service Production Index Wholesale and retail trade | Missing Values in 'stat_value': 1\n",
      "Series: Import Volume Index (exclude Gold) | Missing Values in 'stat_value': 0\n",
      "Series: Nominal Effective Exchange Rate (NEER) | Missing Values in 'stat_value': 2\n",
      "Series: Wholesales Index Durable Goods | Missing Values in 'stat_value': 2\n",
      "Series: Retail Sales Index | Missing Values in 'stat_value': 2\n",
      "Series: Private Investment Index (PII) (Seasonally Adjusted) | Missing Values in 'stat_value': 1\n",
      "Series: Service Production Index Real estate, renting and business activities | Missing Values in 'stat_value': 1\n",
      "Series: Business Sentiment Index of Investment | Missing Values in 'stat_value': 0\n",
      "Series: SET Index | Missing Values in 'stat_value': 1\n",
      "Series: Export Volume Index (exclude Gold) | Missing Values in 'stat_value': 0\n",
      "Series: Export Value Index (THB) | Missing Values in 'stat_value': 1\n",
      "Series: Import Value Index (THB) | Missing Values in 'stat_value': 1\n"
     ]
    }
   ],
   "source": [
    "# Loop through each unique series in rt_df and print missing values count\n",
    "for series in stationary_df['series_name'].unique():\n",
    "    df_series = stationary_df[stationary_df['series_name'] == series].copy()\n",
    "    missing_values_count = df_series['stat_value'].isna().sum()\n",
    "    print(f\"Series: {series} | Missing Values in 'stat_value': {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ca0fa-ec0b-42d2-82e1-ee8aa9d76332",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c5ad78-794f-4bc1-8b5a-5a0090c198a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for imputing missing values with backward fill\n",
    "def impute_leading_with_bfill(df, group_column='series_name', value_column='stat_value'):\n",
    "    \"\"\"\n",
    "    Apply backward fill to leading NaNs in each time series group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): DataFrame with time series.\n",
    "    group_column (str): Column identifying different time series.\n",
    "    value_column (str): Column with the values to be imputed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with leading NaNs backfilled.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "    # Backward fill missing values within each group and store in a new column\n",
    "    df['imputed_val'] = df.groupby(group_column)[value_column].transform(lambda x: x.bfill())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47daa506-fb60-4f99-8c32-e9a19e2ccb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute first missing value with backward fill\n",
    "bfilled_df = impute_leading_with_bfill(stationary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e62947-c766-40cf-aa6a-bc43062a94b7",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab03eed-d880-4f9c-91c9-420e375938bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for outlier detection\n",
    "def detect_outliers(df, group_col='series_name', value_col='sea_adj'):\n",
    "    \"\"\"\n",
    "    Detect outliers using Hyndman's conservative 3*IQR rule applied to the series remainder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): DataFrame with time series including seasonally adjusted values.\n",
    "    group_col (str): Column indicating separate time series (e.g., 'series_name').\n",
    "    value_col (str): Column with seasonally adjusted values to check for outliers (e.g., 'sea_adj').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with outliers flagged.\n",
    "    \"\"\"\n",
    "    # Initialize list to hold flagged series\n",
    "    flagged_dfs = []\n",
    "    \n",
    "    # Process each series in the group\n",
    "    for series, group in df.groupby(group_col):\n",
    "        data = group.copy()\n",
    "        series_values = data[value_col]\n",
    "        \n",
    "        # Compute 3*IQR bounds\n",
    "        q1 = series_values.quantile(0.25)\n",
    "        q3 = series_values.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 3 * iqr\n",
    "        upper_bound = q3 + 3 * iqr\n",
    "\n",
    "        # Identify outliers\n",
    "        outliers = (series_values < lower_bound) | (series_values > upper_bound)\n",
    "        n_outliers = outliers.sum()\n",
    "\n",
    "        if n_outliers > 0:\n",
    "            print(f\"[OUTLIERS] {series}: {n_outliers} outliers detected\")\n",
    "\n",
    "        # Flag outliers without replacing them\n",
    "        data['outlier_flag'] = outliers\n",
    "\n",
    "        # Append flagged series\n",
    "        flagged_dfs.append(data)\n",
    "\n",
    "    return pd.concat(flagged_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "893b8cf7-8ce3-492d-a83b-9bdbd78a5499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUTLIERS] Export Volume Index (exclude Gold): 1 outliers detected\n",
      "[OUTLIERS] Nominal Effective Exchange Rate (NEER): 2 outliers detected\n",
      "[OUTLIERS] Private Consumption Index (Seasonally Adjusted) : 1 outliers detected\n",
      "[OUTLIERS] Private Investment Index (PII) (Seasonally Adjusted): 2 outliers detected\n",
      "[OUTLIERS] Real Effective Exchange Rate (REER): 2 outliers detected\n",
      "[OUTLIERS] Retail Sales Index: 1 outliers detected\n",
      "[OUTLIERS] Retail Sales Index Durable Goods: 2 outliers detected\n",
      "[OUTLIERS] Service Production Index Real estate, renting and business activities: 6 outliers detected\n",
      "[OUTLIERS] Service Production Index Wholesale and retail trade: 1 outliers detected\n"
     ]
    }
   ],
   "source": [
    "# Detect and flag outliers\n",
    "flagged_df = detect_outliers(bfilled_df, value_col = 'imputed_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d4c6b11-ffaa-483b-93af-f16e28208561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with NaN for interpolation\n",
    "outlier_handled_df = flagged_df.copy()\n",
    "outlier_handled_df['cleaned_val'] = outlier_handled_df['imputed_val']\\\n",
    ".where(~outlier_handled_df['outlier_flag'], np.nan)\n",
    "\n",
    "outlier_handled_df['outlier_corrected'] = (\n",
    "    outlier_handled_df\n",
    "    .groupby('series_name')['cleaned_val']\n",
    "    .transform(lambda group: group.interpolate(method='linear', limit_direction='both'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626398d-6922-4958-b271-5668350753ad",
   "metadata": {},
   "source": [
    "## Handle Ragged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f10eeac5-a444-478e-bd59-abc5a13107fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kalman filter\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Define helper function to forecast and fill trailing missing values using a Kalman Filter\n",
    "def forecast_tail_with_kalman(series, series_name=None):\n",
    "    \"\"\"\n",
    "    Forecast and fill tail-end missing values in a time series using the Kalman Filter.\n",
    "    \n",
    "    This function is designed to handle time series where missing values occur \n",
    "    at the end (e.g., due to real-time simulation or delayed release). It uses \n",
    "    the Kalman Filter to forecast and impute those values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series (pd.Series): Time series with potential missing values at the end.\n",
    "    series_name (str, optional): Name of the time series, used for logging purposes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series: Time series with tail-end NaNs replaced by Kalman-based forecasts.\n",
    "    \"\"\"\n",
    "    # Ensure series is float for Kalman filter\n",
    "    series = series.astype(float)\n",
    "\n",
    "    # Count how many NaNs are at the tail of the series\n",
    "    tail_nan_count = series[::-1].isna().cumprod().sum()\n",
    "\n",
    "    # No missing values to forecast\n",
    "    if tail_nan_count == 0:\n",
    "        return series\n",
    "\n",
    "    # Set default name if none provided\n",
    "    name = series_name if series_name is not None else \"Unnamed series\"\n",
    "    print(f\"[KALMAN] {name}: Forecasting {tail_nan_count} tail-end missing values...\")\n",
    "\n",
    "    # Extract observed (non-missing) part of the series\n",
    "    observed = series[:-tail_nan_count]\n",
    "\n",
    "    # Not enough data to estimate the filter\n",
    "    if len(observed) < 2:\n",
    "        return series\n",
    "\n",
    "    # Initialize and fit Kalman Filter using EM algorithm\n",
    "    kf = KalmanFilter(initial_state_mean=observed.iloc[0], n_dim_obs=1)\n",
    "    kf = kf.em(observed.values, n_iter=10)\n",
    "    filtered_state_means, _ = kf.filter(observed.values)\n",
    "\n",
    "    # Forecast missing values step-by-step using the transition matrix\n",
    "    forecasts = []\n",
    "    state = filtered_state_means[-1]\n",
    "\n",
    "    for _ in range(tail_nan_count):\n",
    "        state = kf.transition_matrices @ state\n",
    "        forecasts.append(state[0])\n",
    "\n",
    "    # Replace tail-end NaNs with forecasted values\n",
    "    result = series.copy()\n",
    "    result.iloc[-tail_nan_count:] = forecasts\n",
    "\n",
    "    return result\n",
    "\n",
    "# Define  helper function for applying Kalman Filter\n",
    "def apply_kalman_tail_forecast(df, group_column='series_name', \n",
    "                               input_column='outlier_corrected'):\n",
    "    \"\"\"\n",
    "    Apply Kalman filter to forecast and impute trailing missing values \n",
    "    for each time series group.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): DataFrame with imputed values (e.g., after backward fill).\n",
    "    group_column (str): Column identifying different time series.\n",
    "    input_column (str): Column containing the values to apply Kalman forecasting on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: DataFrame with a new column 'final_val' containing Kalman-imputed values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize output column\n",
    "    df['final_val'] = np.nan\n",
    "    \n",
    "    # Apply Kalman filter to each group\n",
    "    for name, group in df.groupby(group_column):\n",
    "        series = group[input_column]\n",
    "        kalman_imputed = forecast_tail_with_kalman(series, series_name=name)\n",
    "        df.loc[group.index, 'final_val'] = kalman_imputed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0da5ff7-9902-4eda-8467-fa15ab3cb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Kalman filter for filling up missing values\n",
    "imputed_df = apply_kalman_tail_forecast(outlier_handled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e7c07f-879e-45ca-b7df-efbe4aa504b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset imputed_df to keep only certain columns\n",
    "monthly_df_final = imputed_df[['series_code', 'series_name', 'date', 'value']]\n",
    "\n",
    "# Export preprocessed monthly dataset \n",
    "# Uncomment if needed\n",
    "#monthly_df_final.to_csv('preprocessed-dataset-nowcasting-thai-gdp-monthly-jan.csv', \n",
    "#                        index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44867584-c295-46f4-bce7-c3f1a4888ab5",
   "metadata": {},
   "source": [
    "## Get Preprocessed Quarterly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ea5b07-33f8-44f3-9889-c51f7d6ba201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of imputed_df\n",
    "monthly_df = imputed_df.copy()\n",
    "\n",
    "# Add a quarter column\n",
    "monthly_df['quarter'] = monthly_df['date'].dt.to_period('Q')\n",
    "\n",
    "# Aggregate Flows with sum\n",
    "flows = (\n",
    "    monthly_df[monthly_df['data_type'] == 'Flow']\n",
    "    .groupby(['series_code', 'series_name', 'quarter'], as_index=False)['final_val']\n",
    "    .sum()\n",
    "    .rename(columns={'final_val': 'value'})\n",
    ")\n",
    "\n",
    "# Aggregate Index with mean\n",
    "indices = (\n",
    "    monthly_df[monthly_df['data_type'] == 'Index']\n",
    "    .groupby(['series_code', 'series_name', 'quarter'], as_index=False)['final_val']\n",
    "    .mean()\n",
    "    .rename(columns={'final_val': 'value'})\n",
    ")\n",
    "\n",
    "# Combine back together\n",
    "quarterly_df_final = pd.concat([flows, indices], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "468384c9-0374-4efa-85ee-ecaf8065ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessed dataset\n",
    "# Uncomment if needed\n",
    "#quarterly_df_final.to_csv('preprocessed-dataset-nowcasting-thai-gdp-quarterly-jan.csv', \n",
    "#                       index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
